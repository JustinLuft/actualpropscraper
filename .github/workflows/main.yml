# .github/workflows/scraper.yml
name: Web Scraper

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      websites:
        description: 'Comma-separated list of websites to scrape'
        required: false
        default: 'alphacapitalgroup.uk'
      headless:
        description: 'Run in headless mode'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Chrome and ChromeDriver
      run: |
        # Install Chrome browser
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt update
        sudo apt install -y google-chrome-stable
        
        # Install ChromeDriver
        CHROME_VERSION=$(google-chrome --version | cut -d ' ' -f3 | cut -d '.' -f1-3)
        CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
        wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
        sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
        sudo chmod +x /usr/local/bin/chromedriver
        
    - name: Set environment variables
      run: |
        echo "WEBSITES=${{ github.event.inputs.websites || 'alphacapitalgroup.uk' }}" >> $GITHUB_ENV
        echo "HEADLESS=${{ github.event.inputs.headless || 'true' }}" >> $GITHUB_ENV
        echo "OUTPUT_DIR=output" >> $GITHUB_ENV
        echo "TIMEOUT=30" >> $GITHUB_ENV
        
    - name: Create output directory
      run: mkdir -p output
      
    - name: Run scraper
      run: python main.py
      
    - name: Upload results as artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: scraping-results-${{ github.run_number }}
        path: |
          output/*.csv
          scraper.log
        retention-days: 30
        
    - name: Commit and push results (optional)
      if: success()
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add output/*.csv
        if git diff --staged --exit-code; then
          echo "No changes to commit"
        else
          git commit -m "Update scraping results - $(date)"
          git push
        fi

---
# requirements.txt
selenium==4.15.2
pandas==2.1.3
requests==2.31.0
webdriver-manager==4.0.1
lxml==4.9.3
beautifulsoup4==4.12.2

---
# config.json
{
  "websites": [
    "alphacapitalgroup.uk"
  ],
  "output_dir": "output",
  "headless": true,
  "timeout": 30,
  "max_retries": 3,
  "delay_between_requests": 2,
  "user_agents": [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
  ]
}

---
# docker/Dockerfile
FROM python:3.9-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    wget \
    unzip \
    curl \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Install Chrome
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google-chrome.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# Install ChromeDriver
RUN CHROME_VERSION=$(google-chrome --version | cut -d ' ' -f3 | cut -d '.' -f1-3) \
    && CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION") \
    && wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip" \
    && unzip /tmp/chromedriver.zip -d /usr/local/bin/ \
    && chmod +x /usr/local/bin/chromedriver \
    && rm /tmp/chromedriver.zip

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create output directory
RUN mkdir -p output

# Run the scraper
CMD ["python", "main.py"]

---
# docker-compose.yml
version: '3.8'

services:
  scraper:
    build:
      context: .
      dockerfile: docker/Dockerfile
    environment:
      - WEBSITES=alphacapitalgroup.uk
      - HEADLESS=true
      - OUTPUT_DIR=/app/output
      - TIMEOUT=30
    volumes:
      - ./output:/app/output
      - ./config.json:/app/config.json
    networks:
      - scraper-network

networks:
  scraper-network:
    driver: bridge

---
# .gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo

# Logs
*.log
logs/

# Output files
output/*.csv
results/

# Chrome
chrome_user_data/

# Environment variables
.env

# OS
.DS_Store
Thumbs.db

---
# README.md
# Modular Web Scraper Framework

A flexible, modular web scraping framework that can be easily extended to scrape multiple websites and run automatically using GitHub Actions.

## Features

- **Modular Design**: Easy to add new website scrapers
- **GitHub Actions Integration**: Automated scraping on schedule
- **Docker Support**: Containerized execution
- **Robust Error Handling**: Comprehensive logging and fallback strategies
- **Multiple Output Formats**: CSV export with customizable structure
- **Anti-Detection**: User agent rotation and stealth features

## Quick Start

### Local Setup

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd web-scraper
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install ChromeDriver**
   - Download from: https://chromedriver.chromium.org/
   - Or use: `pip install webdriver-manager`

4. **Configure websites**
   Edit `config.json` to add your target websites:
   ```json
   {
     "websites": ["alphacapitalgroup.uk", "example.com"],
     "headless": true,
     "timeout": 30
   }
   ```

5. **Run the scraper**
   ```bash
   python main.py
   ```

### GitHub Actions Setup

1. **Enable Actions**: Go to your repository → Actions tab → Enable workflows

2. **Set up secrets** (if needed):
   - Go to Settings → Secrets and variables → Actions
   - Add any required API keys or credentials

3. **Configure schedule**: Edit `.github/workflows/scraper.yml` to change the cron schedule

4. **Manual trigger**: Use the "Run workflow" button in the Actions tab

### Docker Usage

1. **Build and run with Docker Compose**
   ```bash
   docker-compose up --build
   ```

2. **Or build manually**
   ```bash
   docker build -f docker/Dockerfile -t web-scraper .
   docker run -v $(pwd)/output:/app/output web-scraper
   ```

## Adding New Website Scrapers

1. **Create a new scraper class** inheriting from `BaseScraper`:

```python
# scraper/your_website_scraper.py
from scraper.base_scraper import BaseScraper

class YourWebsiteScraper(BaseScraper):
    def get_base_url(self) -> str:
        return "https://yourwebsite.com/"
        
    def get_selectors(self) -> Dict[str, List[str]]:
        return {
            "account_cards": [".card", ".package"],
            "pricing": [".price", ".cost"]
        }
        
    def extract_account_info(self, element) -> Dict[str, str]:
        # Your extraction logic here
        pass
        
    def parse_page_source(self, page_source: str) -> List[Dict[str, str]]:
        # Fallback extraction logic
        pass
```

2. **Register the scraper** in `ScraperFactory`:

```python
# scraper/scraper_factory.py
from scraper.your_website_scraper import YourWebsiteScraper

class ScraperFactory:
    _scrapers = {
        'yourwebsite.com': YourWebsiteScraper,
        'alphacapitalgroup.uk': AlphaCapitalScraper,
    }
```

3. **Add to configuration**:
```json
{
  "websites": ["yourwebsite.com", "alphacapitalgroup.uk"]
}
```

## Configuration

### Environment Variables

- `WEBSITES`: Comma-separated list of websites to scrape
- `HEADLESS`: Run in headless mode (true/false)
- `OUTPUT_DIR`: Directory for output files
- `TIMEOUT`: Selenium timeout in seconds

### Config File Options

```json
{
  "websites": ["site1.com", "site2.com"],
  "output_dir": "output",
  "headless": true,
  "timeout": 30,
  "max_retries": 3,
  "delay_between_requests": 2
}
```

## Output

Results are saved as CSV files with the following structure:

| Field | Description |
|-------|-------------|
| business_name | Name of the business/company |
| account_size | Trading account size (e.g., 50k, 100k) |
| sale_price | Current sale price |
| funded_full_price | Full funded account price |
| discount_coupon_code | Available discount codes |
| trail_type | Type of trial/evaluation |
| trustpilot_score | Trustpilot rating score |
| profit_goal | Profit target
